# -*- coding: utf-8 -*-
"""Alzheimer's Disease Diagnosis via MRI Scans using CNN Mode

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19oyTnMkcKqUTvu-eAAOltFdnnVwBxflv

# ***upload kaggle credentials file***
"""

!pip install -q kaggle

from google.colab import files
 files.upload()

!rm -r ~/.kaggle
!mkdir ~/.kaggle
!mv ./kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
!kaggle datasets list

"""# **Download and unzip the data**"""

!kaggle datasets download -d sachinkumar413/alzheimer-mri-dataset

! mkdir allData #create directory to unzip the data in
! unzip alzheimer-mri-dataset.zip -d allData

"""# ***Prepare the train, test, and valid directories***"""

!pip install split-folders

import splitfolders
#splitting data with 70% for trainning, 13% for validation, 17% for test
splitfolders.ratio('/content/allData/Dataset', output="allDataSplit", seed=42, ratio=(0.7,0.13, 0.17), group_prefix=None)

"""# ***check the files number***"""

import os
import random
from glob import glob
from pathlib import Path
all_data = [y for x in os.walk('/content/allDataSplit') for y in glob(os.path.join(x[0], '*jpg'))]
len(all_data)

val_data = [y for x in os.walk('/content/allDataSplit/val') for y in glob(os.path.join(x[0], '*jpg'))]
len(val_data)

train_data = [y for x in os.walk('/content/allDataSplit/train') for y in glob(os.path.join(x[0], '*jpg'))]
len(train_data)

import os
import random
from glob import glob
from pathlib import Path
DATASET_DIR = '/content/allDataSplit'
TRAIN_DIR = '/content/allDataSplit/train'
TEST_DIR = '/content/allDataSplit/test'
VAL_DIR='/content/allDataSplit/val'

all_data = [y for x in os.walk(DATASET_DIR) for y in glob(os.path.join(x[0], '*jpg'))]
all_labels = [os.path.basename(os.path.dirname(x)) for x in all_data]

import numpy as np
import matplotlib.pyplot as plt
import matplotlib.image as mpimg

# Sample 25 images from dataset
indices = np.random.randint(0, len(all_data), size=25)
images = [all_data[i] for i in indices]
labels = [all_labels[i] for i in indices]

# Plot the 25 images
plt.figure(figsize=(10,10))
for i in range(len(indices)):
    plt.subplot(5, 5, i + 1)
    image = mpimg.imread(images[i]) # Read image from disk
    plt.imshow(image)
    plt.title(labels[i])
    plt.axis('off')

plt.show()

"""## ***Trainning Data Genreator***"""

from keras.preprocessing.image import ImageDataGenerator
import tensorflow as tf

BATCH_SIZE = 64
HEIGHT = 219
WIDTH = 219

# 1. Construct an instance of the `ImageDataGenerator` class
train_datagen =  ImageDataGenerator(
       rescale = 1.0/255,
      width_shift_range=0.1,
      height_shift_range=0.1,
      rotation_range=20
    )

# 2. Retrieve the iterator
train_generator = train_datagen.flow_from_directory(TRAIN_DIR,
                                                    shuffle = True,
                                                    seed = 7,
                                                    target_size=(HEIGHT, WIDTH),
                                                    batch_size=BATCH_SIZE,
                                                    color_mode='rgb',
                                                    class_mode='categorical',
                                                    subset='training')

"""# ***Validation Data Generator***"""

# 1. Construct an instance of the `ImageDataGenerator` class
VAL_datagen =  ImageDataGenerator(
         rescale = 1.0/255,
      width_shift_range=0.1,
      height_shift_range=0.1,
      rotation_range=20
    )

# 2. Retrieve the iterator
val_generator = VAL_datagen.flow_from_directory(VAL_DIR, shuffle = True,seed = 7,
                                                    target_size=(HEIGHT, WIDTH),
                                                    batch_size=BATCH_SIZE,
                                                    color_mode='rgb',
                                                    class_mode='categorical')

"""## ***Test Data Generator***"""

test_datagen =   ImageDataGenerator(
     rescale = 1.0/255
    )
# 2. Retrieve the iterator
test_generator = test_datagen.flow_from_directory(TEST_DIR,
                                                    target_size=(HEIGHT, WIDTH),
                                                    batch_size=BATCH_SIZE,
                                                    color_mode='rgb',
                                                    class_mode='categorical')

images, labels = next(train_generator)

print(images.shape)
print(labels.shape)

labels[:7]

plt.figure(figsize=(5, 5))
for i in range(9):
    ax = plt.subplot(3, 3, i + 1)
    plt.imshow(images[i])
    plt.axis("off")

from keras.applications.vgg19 import VGG19
from keras.models import Model
from keras.layers import Dense, Flatten, Dropout

# Load model without classification head
base_model = tf.keras.applications.inception_resnet_v2.InceptionResNetV2(include_top = False,
                   weights = 'imagenet',
                   input_shape = (HEIGHT, WIDTH, 3),
                    classifier_activation='softmax')

# Print base model summary
base_model.summary()

base_model.trainable = True
print("Number of layers in the base model: ", len(base_model.layers))

# You can choose to fine-tune some of the final layers:
first_unfrozenLayer = 14

for layer in base_model.layers[:first_unfrozenLayer]:
  layer.trainable =  False


for i, layer in enumerate(base_model.layers):
  print(i, layer.name, layer.trainable)

import keras

# Add new classifier layers
x = Flatten()(base_model.layers[-1].output)
x = Dense(512, activation='relu')(x)
x = Dropout(0.4)(x)
x = Dense(256, activation='relu')(x)
x = Dropout(0.5)(x)
output = Dense(4, activation='softmax')(x)

# Define new model
model = Model(inputs = base_model.inputs, outputs = output)

# Print summary
model.summary()

# Compile
model.compile(optimizer = keras.optimizers.adam_v2.Adam(learning_rate=0.5e-4,beta_1=0.9, beta_2=0.999, amsgrad=True),
            loss = 'categorical_crossentropy',
            metrics = ['accuracy'])

from keras.callbacks import  EarlyStopping

INITIAL_EPOCHS = 100
early_stopping_patience = 3

early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=early_stopping_patience, verbose=1, mode='auto')

history = model.fit(train_generator,  batch_size = 240, epochs=INITIAL_EPOCHS, verbose=1,shuffle=True,validation_data=val_generator, callbacks=early_stopping)

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

loss = history.history['loss']
val_loss = history.history['val_loss']

plt.figure(figsize=(8, 8))
plt.subplot(2, 1, 1)
plt.plot(acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.ylabel('Accuracy')
#plt.ylim([min(plt.ylim()),1])
plt.title('Training and Validation Accuracy')

plt.subplot(2, 1, 2)
plt.plot(loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.ylabel('Cross Entropy')
#plt.ylim([0,1.0])
plt.title('Training and Validation Loss')
plt.xlabel('epoch')
plt.show()

model.evaluate(test_generator)